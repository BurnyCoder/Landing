## Tags
- Metadata: #paper #tool  #processing #long #focus
- Topics: [[Machine learning]] [[Language model]] [[Natural language processing]] [[Transformer]] [[Speed optimization]] [[landing/docs/Contents/Exobrain/Topics/Open source|Open source]] 
- Additional: [[SotA]] 
## Significance
- Extremely reducing the size of LLMs and losing as little accuracy as possible
## Intuitive summary
-  
## Technical summary
- a [[BERT]] variant using 0.3% of neurons needed for same performance using [[fast feedforward networks]]
## Main resources
- [[2311.10770] Exponentially Faster Language Modelling](https://arxiv.org/abs/2311.10770)
	- <iframe src="https://arxiv.org/abs/2311.10770" allow="fullscreen" allowfullscreen="" style="height:100%;width:100%; aspect-ratio: 16 / 9; "></iframe>
## Deep dive
- 
## Brain storming
- 
## Additional resources 
- 

## Related
- 
## AI 
- 

