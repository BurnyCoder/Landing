## Tags
- Metadata: #topic 
- Part of:
- Related: 
- Includes:
- Additional: 
## Significance
- 
## Intuitive summaries
- 
## Definitions
- Some [[Systems science|systems]] in the world seem to behave like “[[Agent|agents]]”: they make consistent decisions, and sometimes display complex goal-seeking behaviour. Can we develop a robust mathematical description of such systems and build provably aligned AI agents?
## Technical summaries
-  
## Main resources 
- 
	- <iframe src="https://en.wikipedia.org/wiki/" allow="fullscreen" allowfullscreen="" style="height:100%;width:100%; aspect-ratio: 16 / 5; "></iframe>
## Landscapes
- [Agent Foundations - AI Alignment Forum](https://www.alignmentforum.org/tag/agent-foundations)
## Contents
- 
## Deep dives
- 
## Brain storming
- 
## Additional resources  
- [Why Agent Foundations? An Overly Abstract Explanation — LessWrong](https://www.lesswrong.com/posts/FWvzwCDRgcjb9sigb/why-agent-foundations-an-overly-abstract-explanation)
## Related
- 
## Related resources  
- 
## AI 
- 
## Additional metadata 
-  #processed #processing #toprocess #important #short #long #casual #focus
- Unfinished: #metadata #tags