## Tags
- Metadata: #topic 
- Part of: [[Artificial Intelligence]] [[Machine learning]] [[Risks of artificial intelligence]] [[Risks]]
- Related: 
- Includes:
- Additional: 
## Significance
- 
## Intuitive summaries
- 
## Definitions
- 
## Technical summaries
-  
## Main resources 
- 
	- <iframe src="https://en.wikipedia.org/wiki/AI_safety" allow="fullscreen" allowfullscreen="" style="height:100%;width:100%; aspect-ratio: 16 / 5; "></iframe>
## Landscapes
- Methods
	- [[Mechanistic interpretability]] 
		 -  ![[Mechanistic interpretability#Definitions]]
	- [[Readteaming]]
		- ![[Readteaming#Definitions]]
	- Evaluating dangerous [[Capability|capabilities]]
		- ![[Capability#Definitions]]
	- [[Process supervision]]
		- ![[Process supervision#Definitions]]

- [Alex Turner’s landscape](https://www.youtube.com/watch?v=02kbWY5mahQ)
	- ![US presidents rate AI alignment agendas - YouTube](https://www.youtube.com/watch?v=02kbWY5mahQ)
		- [[Mechanistic interpretability]]
		- [[Agent foundations]]
			- ![[Agent foundations#Definitions]]
		- [[Cognitive Emulation]] - build predictably boundable systems ([Cognitive Emulation: A Naive AI Safety Proposal — LessWrong](https://www.lesswrong.com/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal))
		- [[Shard theory]] 
			- ![[Shard theory#Definitions]]
		- [[Infrabayesianism]] - [Infra-Bayesianism - LessWrong](https://www.lesswrong.com/s/CmrW8fCmSLK7E25sa)
		- [[Eliciting latent knowledge]] -  How can we train this model to report its latent knowledge of off-screen events? [Eliciting latent knowledge. How can we train an AI to honestly tell… | by Paul Christiano | AI Alignment](https://ai-alignment.com/eliciting-latent-knowledge-f977478608fc)
	
## Contents
- 
## Deep dives
- 
## Brain storming
- 
## Additional resources  
- 
## Related
- 
## Related resources  
- 
## AI 
- 
## Additional metadata 
-  #processed #processing #toprocess #important #short #long #casual #focus
- Unfinished: #metadata #tags