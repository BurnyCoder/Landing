## Tags
- Metadata: #topic 
- Part of: [[Machine learning]]
- Related: 
- Includes:
- Additional: 
## Significance
- 
## Intuitive summaries
- 
## Definitions
- [[Mathematics|Mathematical]] and [[physics]] based theory of [[artificial Intelligence]], such as [[machine learning]], such as [[deep learning]] with [[artificial neural network]]s or [[reinforcement learning]].
## Technical summaries
-  
## Main resources 
- 
## Landscapes
- [[Statistical learning theory]]
	- [Statistical learning theory - Wikipedia](https://en.wikipedia.org/wiki/Statistical_learning_theory)
	- Includes: [[Statistics]], [[Functional analysis]]
- [[Deep Learning]]
	- [[The Principles of Deep Learning Theory]]
		- [\[2106.10165\] The Principles of Deep Learning Theory](https://arxiv.org/abs/2106.10165)
		- [Introduction to Deep Learning Theory - YouTube](https://www.youtube.com/watch?v=pad023JIXVA)
		- ![Introduction to Deep Learning Theory - YouTube](https://www.youtube.com/watch?v=pad023JIXVA)
		- [A New Physics-Inspired Theory of Deep Learning | Optimal initialization of Neural Nets - YouTube](https://www.youtube.com/watch?v=m2bXL5Z5CBM)
		- ![A New Physics-Inspired Theory of Deep Learning | Optimal initialization of Neural Nets - YouTube](https://www.youtube.com/watch?v=m2bXL5Z5CBM)
		- Includes: [[Linear algebra]], [[Calculus]] ([[Multivariable Calculus]]), [[Probability theory]], [[Statistics]], [[Differential equations]], [[Information theory]], [[Optimization theory]], [[Physics]] ([[Theoretical physics]]) ([[Statistical mechanics]], [[Quantum mechanics]]) ([[effective field theory]], [[renormalization group]]), [[Functional analysis]], [[Bayesian statistics]], [[Signal processing]], [[Kernel methods]]
		- [\[1910.00359\] Truth or Backpropaganda? An Empirical Investigation of Deep Learning Theory](https://arxiv.org/abs/1910.00359)
	- [[Geometric deep learning]]
		- [Geometric Deep Learning - Grids, Groups, Graphs, Geodesics, and Gauges](https://geometricdeeplearning.com/)
		- [ICLR 2021 Keynote - "Geometric Deep Learning: The Erlangen Programme of ML" - M Bronstein - YouTube](https://www.youtube.com/watch?v=w6Pw4MOzMuo&t=630s&pp=ygUXZ2VvbWV0cmljIGRlZXAgbGVhcm5pbmc%3D)
		- ![ICLR 2021 Keynote - "Geometric Deep Learning: The Erlangen Programme of ML" - M Bronstein - YouTube](https://www.youtube.com/watch?v=w6Pw4MOzMuo&t=630s&pp=ygUXZ2VvbWV0cmljIGRlZXAgbGVhcm5pbmc%3D)
		- [GEOMETRIC DEEP LEARNING BLUEPRINT - YouTube](https://www.youtube.com/watch?v=bIZB1hIJ4u8&pp=ygUXZ2VvbWV0cmljIGRlZXAgbGVhcm5pbmc%3D)
		- ![GEOMETRIC DEEP LEARNING BLUEPRINT - YouTube](https://www.youtube.com/watch?v=bIZB1hIJ4u8&pp=ygUXZ2VvbWV0cmljIGRlZXAgbGVhcm5pbmc%3D)
		- Includes: [[Group theory]] ([[Symmetry]]), [[Differential geometry]], [[Topology]], [[Harmonic analysis]], [[Functional analysis]], [[Probability theory]], [[Category theory]], [[Algebra]], [[Graph theory]], [[Geometry]], [[Computational geometry]]
	- [[Spline Theory of Deep Learning]]
		- [A Spline Theory of Deep Learning](https://proceedings.mlr.press/v80/balestriero18b.html)
		- [Ahmed Imtiaz Humayun on the spline theory of NNs #machinelearning - YouTube](https://youtube.com/shorts/njlCP32Y-18?si=gGrS91TND6i_t8qO)
		- Includes:[[Spline theory]], [[Approximation theory]], [[Linear algebra]], [[Optimization theory]], [[Information theory]], [[Signal processing]], [[Functional analysis]]
	- [[Categorical Deep Learning]]
		- [\[2402.15332\] Categorical Deep Learning: An Algebraic Theory of Architectures](https://arxiv.org/abs/2402.15332)
		- [WE MUST ADD STRUCTURE TO DEEP LEARNING BECAUSE... - YouTube](https://www.youtube.com/watch?v=rie-9AEhYdY)
		- ![WE MUST ADD STRUCTURE TO DEEP LEARNING BECAUSE... - YouTube](https://www.youtube.com/watch?v=rie-9AEhYdY)
		- Generalizing [[Geometric deep learning]], [[Topological data analysis]]
		- Includes: [[Category theory]], [[Algebra]], [[Abstract algebra]], [[Group theory]], [[Topology]], [[Universal algebra]], [[Type theory]], [[Linear algebra]], [[Automata theory]], [[Logic]]
	- [[Singular learning theory]]
		- [Singular Learning Theory - LessWrong](https://www.lesswrong.com/s/mqwA5FcL6SrHEQzox)
		- [singular learning theory in nLab](https://ncatlab.org/nlab/show/singular+learning+theory)
		- [Singular Learning Theory - Working Session 1 - YouTube](https://www.youtube.com/watch?v=cuPeJkeiYsI)
		- Includes: Applying [[Algebraic Geometry]] to [[Statistical learning theory]]
		- [My Criticism of Singular Learning Theory — LessWrong](https://www.lesswrong.com/posts/ALJYj4PpkqyseL7kZ/my-criticism-of-singular-learning-theory)
	- [[Shard theory]]
		- [Shard Theory: An Overview — LessWrong](https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview)
		- Includes: [[Utility theory]], [[Game theory]], [[Optimization theory]], [[Probability theory]]
	- [[Neural tangent kernel]]
		- [Neural tangent kernel - Wikipedia](https://en.wikipedia.org/wiki/Neural_tangent_kernel)
		- Includes: [[Kernel methods]], [[Linear algebra]], [[Calculus]], [[Probability theory]] , [[Statistics]] , [[Optimization theory]], [[Functional analysis]]
	- [[Mathematics of adversarial deep learning]], deep learning being unstable despite the existence of stable neural networks
		- [\[2109.06098\] The mathematics of adversarial attacks in AI -- Why deep learning is unstable despite the existence of stable neural networks](https://arxiv.org/abs/2109.06098)
		- [Mathematical Theory of Adversarial Deep Learning](openreview.net/pdf?id=fDDYcOA7h6)
			- [Mathematical Theory of Adversarial Deep Learning Presentation](mmrc.iss.ac.cn/\~xgao/paper/csiam2022.pdf)
	- [Why More Is More (in Artificial Intelligence) | by Manuel Brenner | Towards Data Science](https://towardsdatascience.com/why-more-is-more-in-deep-learning-b28d7cedc9f5)
		- How [[Deep Learning]] [[generalization|Generalizes]]
		- [FLAT MINIMA](bioinf.jku.at/publications/older/3304.pdf) - sharp minima tend to generalize poorly compared to their flat counterparts and gradient descent is more **likely to run into flat minima** during optimization
	- [[Fundamental limits of neural networks inspired by Gödel and Turing]]
		- [Mathematical paradox demonstrates the limits of AI | University of Cambridge](https://www.cam.ac.uk/research/news/mathematical-paradox-demonstrates-the-limits-of-ai)
		- [The difficulty of computing stable and accurate neural networks: On the barriers of deep learning and Smale’s 18th problem | PNAS](https://www.pnas.org/doi/10.1073/pnas.2107151119)
	- [[Neural operators]]
		- [Neural operators - Wikipedia](https://en.wikipedia.org/wiki/Neural_operators)
			- [\[2010.08895\] Fourier Neural Operator for Parametric Partial Differential Equations](https://arxiv.org/abs/2010.08895)
- Idealizations
	- [[AIXI]]
		- ![[AIXI#Technical summaries]]
	- [[Godel machine]]
		- ![[Godel machine#Technical summaries]]
- Reverse engineering
	- [[Mechanistic interpretability]]
## Contents
- 
## Deep dives
- 
## Brain storming
- Categorical geometric effective deep learning
## Additional resources  
- Physics based or informed [[Machine learning]] 
	- Brunton
		- [\[2311.00212\] A Unified Framework to Enforce, Discover, and Promote Symmetry in Machine Learning](https://arxiv.org/abs/2311.00212)
		- [Physics Informed Machine Learning: High Level Overview of AI and ML in Science and Engineering - YouTube](https://www.youtube.com/watch?v=JoFW2uSd3Uo&list=PLMrJAkhIeNNQ0BaKuBKY43k4xMo6NSbBa&index=1)
	- [Energies | Free Full-Text | A Review of Physics-Informed Machine Learning in Fluid Mechanics](https://www.mdpi.com/1996-1073/16/5/2343)
	- [\[2109.05237\] Physics-based Deep Learning](https://arxiv.org/abs/2109.05237)
	- [Welcome … — Physics-based Deep Learning](https://physicsbaseddeeplearning.org/intro.html)
- Connecting [[Differential equations]] and [[Deep Learning]] 
	- [GitHub - Zymrael/awesome-neural-ode: A collection of resources regarding the interplay between differential equations, deep learning, dynamical systems, control and numerical methods.](https://github.com/Zymrael/awesome-neural-ode?tab=readme-ov-file#theory-and-perspectives)
	- [\[1911.00502\] Review: Ordinary Differential Equations For Deep Learning](https://arxiv.org/abs/1911.00502)
	- [Neural networks with infinite layers — Artificial Intelligence, Data Science, Machine learning | by Fra Gadaleta | Medium](https://frag.medium.com/neural-networks-with-infinite-layers-artificial-intelligence-data-science-machine-learning-f5a03a32456a)
	- [\[2002.08071\] Dissecting Neural ODEs](https://arxiv.org/abs/2002.08071)
	- [\[1908.10920\] Deep Learning Theory Review: An Optimal Control and Dynamical Systems Perspective](https://arxiv.org/abs/1908.10920)
## Related
- 
## Related resources  
- 
## Explanation by AI 
- 
## Landscapes by AI 
- 
## Deep dives by AI 
- Neural networks are biased towards learning simple, generalizing solutions due to the following key reasons:

1. The parameter-function map is heavily biased towards simple functions. There is an exponential bias in the parameter-function map of neural networks that strongly favors simple functions. The number of parameters that produce a given simple function is much larger than the number of parameters producing a complex function[1][3].

2. SGD and its variants exhibit an extreme simplicity bias. Standard training procedures like SGD have a strong tendency to find simple models, even in the presence of more complex predictive features in the data. Neural networks can rely exclusively on the simplest features and remain invariant to all complex features[2].

3. The simplicity bias is present at initialization, before any training. Even at random initialization, neural networks are more likely to express simple functions compared to complex ones. Changing this initialization probability is difficult and the simplicity bias is not too sensitive to the type of initialization used[3].

4. Compatible inductive biases lead to better generalization from limited data. When the simplicity bias of the neural network aligns well with the learning task, it facilitates good generalization performance even from a small number of training examples[5].

In summary, the combination of the exponentially biased parameter-function map, the extreme simplicity bias of SGD, and the presence of this bias at initialization, all contribute to neural networks learning simple patterns that generalize well to unseen data. This simplicity bias is a key factor behind their success[1][2][3].

Citations:
[1]  [\[1805.08522\] Deep learning generalizes because the parameter-function map is biased towards simple functions](https://arxiv.org/abs/1805.08522)
[2]  [\[2006.07710\] The Pitfalls of Simplicity Bias in Neural Networks](https://arxiv.org/abs/2006.07710)
[3]  [Deep Neural Networks are biased, at initialisation, towards simple functions | by Chris Mingard | Towards Data Science](https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99)
[5]  [Generalization and Inductive Bias in Neural Networks - YouTube](https://www.youtube.com/watch?v=JrMdDzbeL_w)

Neural networks are biased towards learning simple, generalizing solutions despite being highly expressive models capable of fitting even random input-output mappings. This bias towards simplicity can be understood in several ways:

1. Low frequency bias: Deep ReLU networks are biased towards learning low frequency functions, meaning they tend to capture global patterns and avoid local fluctuations[4]. Intuitively, this aligns with the observation that over-parameterized networks find simple patterns that generalize across data samples.

2. Simplicity bias in the parameter-function map: The mapping from the space of network parameters (weights) to the space of functions (input-output mappings) is heavily biased towards functions with low descriptional complexity[5]. Simple functions are exponentially more likely to occur upon random sampling of network parameters compared to complex functions.

3. Constraints and geometry of decision boundaries: By constraining each output of the neural network to be a convex combination of its inputs, certain desirable geometries of the decision boundaries can be achieved[1]. These constraints may guide the network towards learning simpler, generalizing solutions that lie in a constrained subspace of the hypothesis space.

4. Optimization towards simple solutions: Even with random initialization, iterative optimization of neural network parameters tends to converge towards simple, explicit solutions[2]. This suggests an inherent bias in the optimization process towards finding simple, generalizing solutions.

In summary, the combination of architectural biases, simplicity bias in the parameter-function map, constraints on decision boundaries, and optimization dynamics all contribute to neural networks' tendency to learn simple, generalizing solutions despite their high expressivity.

Citations:
[1]  [\[PDF\] Towards Understanding and Improving the Generalization Performance of Neural Networks | Semantic Scholar](https://www.semanticscholar.org/paper/bb494b7d150ef15a45de24a7f02560c7fae3751f)
[2]  [\[2311.07498\] Reducing the Need for Backpropagation and Discovering Better Optima With Explicit Optimizations of Neural Networks](https://arxiv.org/abs/2311.07498)
[3]  [\[2212.09993\] Are Deep Neural Networks SMARTer than Second Graders?](https://arxiv.org/abs/2212.09993)
[4]  [\[1806.08734\] On the Spectral Bias of Neural Networks](https://arxiv.org/abs/1806.08734)
[5]  [Simplicity bias in the parameter-function map of deep neural networks | Semantic Scholar](https://www.semanticscholar.org/paper/3e82aa414f4f5ecbf2a7601400cbeaed29ae1278)
## AI 
- 
## Additional metadata 
-  #processed #processing #toprocess #important #short #long #casual #focus
- Unfinished: #metadata #tags