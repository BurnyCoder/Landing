## Tags
- Metadata: #paper #toprocess
- Topics: [[Machine learning]] [[Language model]] [[Natural language processing]] [[Transformer]]
- Additional: [[SotA]] [[Open Source]]
## High level implications
- Extremely reducing the size of LLMs and losing as little accuracy as possible
## Summary
-  a [[BERT]] variant using 0.3% of neurons needed for same performance using [[fast feedforward networks]]
## Article
- [[2311.10770] Exponentially Faster Language Modelling](https://arxiv.org/abs/2311.10770)
- <iframe src="https://arxiv.org/abs/2311.10770" allow="fullscreen" allowfullscreen="" style="height:50%;width:50%; aspect-ratio: 16 / 9; "></iframe>

## Deep dive
- 
## Brain storming
- Feel the [[AGI]]
## Related
## AI 
- (GPT summary?)